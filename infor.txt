Informative explanation



Hello and welcome to this project Getting Started With Natural Language Processing in Python. Natural Language Processing is an area of computer science and artificial intelligence concerned with the interactions between computers and human languages, in particular how to program computers to process large amounts of natural language data. In this project, we aren't going to have one main output, instead we are going to cover all of the basics, including tokenizing, part of speech tagging and chunking. This will open the door to more complicated concepts later on, such as machine learning and natural language process and how to use scikit-learn classifiers with NLTK. In this project, we are going to go through, I'll lead you step by step, line by line, how to tokenize words, remove stopwords, how to produce word stems, part of speech tagging and then chunking and chinking, as well as ultimately a simple text classification using a support vector classifier in NLTK. I hope you enjoy the project and I can't wait to go through all of these lines with you line by line, step by step, so that you can develop a strong foundation in Natural Language Processing. [No Audio]



Hello everybody, and welcome to this tutorial on Natural Language Processing. We're going to have some cool information today, and we're going to cover the foundations of Natural Language Processing so that we can get set up to do more complicated machine learning tasks with language based processing in the future. Another thing we're going to do in this tutorial, which is really exciting for me, I think I'm going to be an early adopter, is we're going to use the JupyterLab. So just a few days ago, JupyterLab released an extension to their Jupyter notebooks, which we've been using in all of our previous projects called JupyterLab, which is kind of an improved IDE of Jupyter notebooks. And so I've got this installed, and it's what I'm going to be using today. I encourage you guys to do it as well. We'll see some of the cool features that it provides, some of the benefits over just standard Jupyter notebooks or not necessarily benefits, but differences. So this is kind of the next step in the Jupyter notebooks line. So installing it's really easy. You can use conda or pip install. Just like most things that we've been installing. Conda has been going kind of slow for me in the past few days. So I'm just going to use the pip install, and I actually already installed this. So this is probably going to tell me that I have no updates needed. But that's okay, we'll run it anyways just for your guys' benefit. So, requirement already satisfied, but if you need to do do those installs, you can do it here. So one thing that we're going to be doing today is we're going to be using the NLTK toolkit. So this is a Natural Language Processing Toolkit in Python. So we also have to do pip install nltk. This is something that we haven't used in previous projects. Once again, I already have this installed, so we're good to go there. But make sure that you get this and make sure it installs correctly, because otherwise you will not have any of the functions that we are using. All right, so now, just like Jupyter notebooks, opening up the JupyterLab is really simple. All we have to do is type in jupyter lab, and if jupyter is in our path, we can just click Enter and this will start pulling this up. So we'll wait for this for a second. It takes a little longer, depending on the number of kernels you have. I've got a couple because I set up a couple of different conda environments. We haven't talked about conda environments much yet. However, they're a good way to kind of control what packages you have installed. So maybe we'll get into that in a future project. All right, so here comes the JupyterLab, opening up on the localhost. And here we go. I actually have a couple of projects in here. The start of our Natural Language Processing. I was building this earlier, but we're going to start a new notebook here. So it comes with this convenient Launcher. So this is kind of the difference between just Jupyter notebooks and the JupyterLab here is that you have your directory over here. So whatever directory you have open, [No Audio] you can see over here. Since I started this in my Users\brend\Tutorial folder, that is what's showing over here. So we've got the previous project we did, which was Classifying DNA Sequences, and then this one, which is Natural Language Processing. We have a launcher so we can look, we can launch a Console, we can also do notebooks. And the benefit of JupyterLabs is it can display a lot of other things as well. CSV files, text files, and it also has extensions where you could process other data. You've got convenient tabs over here so you can see which kernels are running or which notebooks are running, Cell Tools, Commands, that type of thing. So really convenient. And I also like this Console feature. If you were wanting to do some quick testing, this is just the standard IPython Console, but we're not going to use that today. We're going to go into another default notebook and let's go ahead and rename this. Call it Natural Language Processing. All right, so we're good to go there. All right, so like I said, we're going to be using NLTK. So hopefully you already did the pip install nltk. But just in case, let's go ahead and import this now and make sure this imports correctly. And actually we can go ahead and do some other imports just to show what we are going to be using. And it did, you see this one here. So nltk has been installed correctly and it was able to import. So let's do a print. We'll do our standard just to make sure that we are on the same page here. [Author Typing] So we have that and we're also going to do the, let's print out the nltk version. [Author Typing] There we go. And the last one, the sklearn. [Author Typing] All right, let's go ahead and click Enter here. So here we go. NLTK 3.2.3, Python 2.7 of course, what we've been using in the past. You can kind of see there's some slightly different formatting here than the previous Jupyter notebooks. You can control all of that in the Settings tab, even Jupyter Themes. Some people like to have the dark. I like the light personally, I think it's a little bit easier to look at, but whatever you prefer, you can change around up there, text formatting, that type of thing. All right, so here we go. So we have the nltk imported, but we actually don't have everything that we need downloaded. So if we type nltk.download, this is going to open up a sort of download GUI that you shall see momentarily that we can use to kind of install different packages that we're going to need. So here you see it opened up down here. And this is the NLTK Downloader. So the green is what you've already installed. So I've already installed some of the popular packages from the collections. All we really need is the popular here. So go ahead and select popular and then just select Download and that will go ahead. From the Corpora, this is kind of like bodies of text. So I haven't installed all of these yet. Some of them that will do automatically when you install the popular shakespeare. It's got a lot of shakespeare in here. The State of the Union Addresses, we're actually going to be using those. So go ahead and get those installed as well as the Universal Declaration of Human Rights, we're going to use that as an example. But you can use any of these that you like. These, like I said, are just big documents of text that we'll be using in this tutorial. All right, so first let's go over some basics of vocabulary in Natural Language Processing. So I'm going to paste this in here. So another cool thing oh, here we go. Let's pull this back off of the site. Another cool thing in Jupyter Notebooks that we haven't really talked about is the ability to switch from Markdown to Code. So down here, if I switch this to Markdown, we're immediately going to have some different text and we'll see how this formats in a second. So Markdown text is pretty standard across the Python community. So it's really nice that you can use these cells in here. And so all it's really going to do is it's going to process it as text instead of as a code cell. So that's really handy if you're building a notebook that you want to share with other people because you can put in well commented code, as you might have seen in some of the source code examples that you have gotten through Eduonix. So here we have some, some three basic definitions. The Corpus, which is a body of text, singular. Corpora is the plural of this. So you saw that in the NLTK downloader a second ago. A Lexicon is going to be like the words and their meanings. So for example, the English dictionary. However, other fields are going to have different lexicons. So you know, when you say something to a hockey player, like, let's say you say icing, that's going to mean something way different than if you're talking to a cake maker. And that's just what Lexicon means, the words in their specific context. So Token here is like an entity that is split up based on certain rules. So for example, here, if each word is a Token, when a sentence is tokenized into words, or you know a sentence would be a token if you were splitting the sentences apart out of a paragraph. So that's actually what we're going to start with and it's going to be tokenizing words. So we're going to do that in a SEC. In this project, we are actually going to be covering tokenizing, which again is the splitting of the sentences and words from a body of text. We're going to be doing part of speech tagging. A little bit later we'll look at chunking as well as things like stemming. And then this foundation is going to open up the door for machine learning in conjunction with Natural Language Processing. So we're going to be doing a little bit of machine learning in NLP or Natural Language Processing. And we're going to look at how to tie in the scikit-learn packages with NLTK. And then ultimately we're going to work up to training a classifier with a given data set. But ultimately these are basically just foundational steps. So ultimately we're going to look up to performing live streaming, sentiment analysis and that's going to be pretty cool. So that's what we're building up to. That's going to take us a couple of projects. But for now, let's just get started with tokenizing these words. So let's go. We'll have from nltk.tokenize import sent_tokenize as well as word_tokenize. So in our definition above, we saw that we could split into sentences or words. These are going to do both of that. Both of those for us. So let's get some example text. Say Hello students, how are you doing today? I've been watching the Olympics. The Olympics are inspiring and Python is awesome. You look great today. All right, here is our sentence, just some example text. Let's go ahead and print out the sentence tokenize of this text. Go ahead and click Shift+Enter. So as you can see, this is split this into different sentences. We have three tokens here. Hello students, how are you doing today? And then we have the next one. The Olympics are inspiring, and Python is awesome as well as You look great today, of course. So let's go ahead and compare that perhaps to the word_tokenize of this same text. And we click Shift+Enter here. You see that splitting it up into the individual words. Hello, students, how are you doing today? As well as you also see that you get the punctuations here. So we have a question mark as its individual token, comma, period, that type of thing. So the word_tokenizer will split things up based on the words sentence_tokenizer, split them into sentence. So this is going to be essentially one of the first preprocessing steps that we are always going to do when we're doing Natural Language Processing. All right, moving on. So when we're using Natural Language Processing, our goal is to perform some analysis or processing so that a computer can respond to the text appropriately. So the process of converting data to something a computer can understand is referred to as preprocessing, which I just mentioned. One of the major forms of preprocessing is going to be filtering out useless data so in Natural Language Processing, useless words or data are referred to as stop words. So let's look at removing stop words from NLTK. So we'll add a comment here, removing stop words, which is useless data. So from nltk.corpus, let's go ahead and import the stopwords. And, so If we ever import something and it tells you that you don't have it installed, you can always go back to this nltk.download and pull up that same GUI and install the packages or the corpus that you need. All right, so let's print this out. print set stopwords.words and we'll do this for the english language. [no audio] All right, ohh so, No module named nltk.corpus. Probably because I typed ntlk. That's not going to work. There we go. Much better. All right, so here we see all of the stop words. Well, maybe not all of the stopwords in the English language, but some words that NLTK has identified as not being that helpful to understanding the entire sentence. So let's look at how we could use these stopwords to kind of filter out a sentence. Some of these just intuitively, you might not think that the word all or perhaps before is useless. However, we'll actually see that from a computer's perspective, they kind of are. So let's get some example. So, example here, let's just say, This is some sample text, showing off the stop words filtration. All right, so here's our example. Then we're going to do stop_words. We got to define these again. stop_words = set stopwords.words. English is the ones we want here. All right, there's that. Let's do the word_tokens. So we're going to have to tokenize these words. And since we want the individual words, we'll use the word_tokenizer [Author Typing] and we'll do this to our example sentence. Then we'll get down to filtered_sentence is going to equal w. So we're going to do a for-loop, an inline for-loop here. So w or word, for w in word_tokens. So it's going to loop through our word_tokens here, if not w in stop_words. So it's going to compare it to the list of stop_words. And if it's not in the stop_words list or the set, it's going to add it to this filtered sentence here. [author typing] So another way, or let's just define this in a slightly different way. We're going to show that we could do the same thing with a for-loop here [Author Typing] if w not in stop_words. filtered_sentence.append w. All right, so this is just two different ways that we can do this. So this would actually do the same thing. Let's go ahead and print the word_tokens. [no audio] Print the filtered_sentence as well. Let's actually split this up. Let's just call it a sentence and so we can actually show that they do the same thing. I just wanted to add an example of an inline for-loop here so that we can see those, because this is actually probably better Python programming than using a for-loop like this. But at the same time, this is more understandable, it's more readable. So that has its benefits as well. So here we go. So this is some sample text showing off the stop_words filtration. That's our sentence above tokenized. Those are word_tokens. And we've done these two different filtering steps which produce the same results. This sample text comma showing stop words filtration, period. So we've removed is, is actually the only stop word in this and the showing off the so we've removed a couple of words here, and we've produced sentences that are both much simpler and don't have this kind of useless data in them. So the benefits of shortening sentences like this is that it's going to substantially speed up our lookup if we're trying to go back and find a similar text. So computationally, it's better to use these filtered_sentences. So kind of filtering out these stop_words. Very common and important first step in preprocessing for Natural Language Processing. All right, now we're going to move on to looking at stemming words with NLTK. So we'll do stemming words with NLTK. All right, stemming is going to be an attempt to normalize sentences, and it is another preprocessing step that we can perform. Excuse me. So in the English language, different variations of words and sentences often have the same meaning. So stemming is a way to account for these variations. Furthermore, it will help us just like the filtering, the stop_word filtering above, it's going to help us shorten the sentences and shorten our lookup. So, for example, we could consider two sentences, and the first being, I was taking a ride on my horse. The second being, I was riding my horse. Those both mean the same thing. So if we're able to shorten those down into a similar stem, that is going to be very beneficial for us. So those sentences mean the same thing. They have the same tense. However, that isn't intuitively understood by the computer. So to account for all the variations of words in the English language, we're going to use the PorterStemmer. And this has actually been around for a long time, I believe 1979. But we can import this from the nltk.stem import PorterStemmer. So it's nicely implemented in NLTK and it's like most things, really easy to use. We just have to initiate the PorterStemmer here. Let's get some example_words. [Author Typing] So we'll do a list here. Let's do ride. We're talking about horses, riding, rider, rides. All right, so here's our example words, and then we can simply do for w in example_words. Sorry, let me scroll down here. We'll print off ps.stem w, where this ps is our PorterStemmer. All right, so click enter here, and I keep typing ntlk that's okay. All right, here we go. So you see here we've split some of these rider. It actually doesn't change because it actually has a slightly different meaning. But riding, rides and ride are all just condensed down to the word ride. So this is really useful. We've taken the stem out of these words and kind of removed the conjugations or the tenses. So this is going to be quicker and easier for us to work with. So now let's take a look at stemming an entire sentence, [Author Typing] because it's going to produce some kind of odd results for us. So it's a good idea to look at those. When riders are riding their horses, they often think of how cowboys rode horses. All right, here we go. Ohh, I've got an extra punctuation there. So first things first word_tokenizer, [Author Typing] so there's that. And then just like above, we'll do for w in words. [Author Typing] print ps.stem(w), Shift+Enter. Here we go. So you notice, first things first, it returns all lowercase. So that's helpful removes the capitalization. And then, like, riders is just a stem rider. Some of these don't get changed at all. Riding gets changed to ride, horses, you actually lose both the E and the S still have our comma. Cowboy loses an s, horses again, rode doesn't get changed to ride. That's still past tense. So here we go. Stemming of an entire sentence, we've kind of condensed these down. So now that we have these stems, we were able to tokenize them, produce these stems. We can move on to part of speech tagging, but we're actually going to get to part of speech tagging in the next video because otherwise this video would be way too long. So, Congratulations you have started building your foundation. We now know how to tokenize words and we know how to get back the stems. And we're also all set up in JupyterLabs to continue working with NLTK. So keep listening. This is just the beginning. Thank you very much. [No Audio]




[No Audio] So welcome back to the second video on this project on Natural Language Processing. In the previous video, we looked at getting started with NLTK. We looked at removing stop words, producing word tokens, stemming an entire sentence. So now we can move on to part of speech tagging. Part of speech tagging means labeling words as nouns, verbs, adjectives, or whatever. So even better, NLTK can handle tenses. While we're at it, we're also going to be importing a new stem tokenizer called the Punkt Sentence Tokenizer. So this tokenizer is actually really impressive. It's capable of unsupervised learning. So what that means is that it can be trained on any body of text in order to kind of fit its tokenizing into that specific body of text. So for this example, let's use the Universal Declaration of Human Rights. We'll just show that here. [No Audio] So we'll import the udhr, and then we'll print out the udhr.raw English-Latin1. So this is the text. And remember, if you didn't install this earlier, oh, went too high. Here we go. Let's move this down. What you can do is the nltk.download, parentheses, click Enter on that, bring this up under Corpora or our text packages here. You can come down here and get this udhr right here, which is the Universal Declaration of Human Rights. All right, so if we do Shift+Enter on this, we actually see you know, this awesome document. All human beings are born free and equal in dignity and rights, etc, etc. It's printed out for us. So this is kind of how these documents are going to be loaded into Python. So I'm just using this Universal Declaration of Human Rights as an example. We're actually going to be looking at some other sample training text for this Punkt Sentence tokenizer because it has two similar examples. We have in the corpus, [Author Typing] we have the State of the Union addresses. And this is actually really nice because we have both the 2005 and 2006 state of the union addresses. So since they were both given by George Bush, there are similar language in each. So we're able to train the sentence tokenizer on his first speech and then use it on the second speech. And it'll actually be much better than a generic tokenizer [Author Typing] since we did train it on some of George Bush's previous speeches. [Author Typing] So here's the Punkt Sentence Tokenizer. And then, so we'll have our train_text. And this is going to be the state_union.raw 2005-George Bush, GWBush.txt and then so sample_text where we're actually going to be using this train tokenizer process [Author Typing] is this 2006-GWBush.txt. [Author Typing] All right, so there we go. We have those. We'll click shift+enter. You see, very quickly it's able to get these in. If we print these out just as an example, you see, we got the whole speech here, just like we have above with the Universal Declaration of Human Rights. This is a little bit longer. It says entire State of the Union. But that's all right. One thing you can do in these is if you right click on these cells, you can clear the outputs. That's pretty nice if you have a really long output and you don't want it to show. So I'm just going to do that. We're just looking at that example there. All right, but now that we have some text, [Author Typing] we can train the PunktSentenceTokenizer. [Author Typing] All right, so really simple. We'll have our custom_sent_tokenizer. This is going to be a class instance. [Author Typing] And all we have to do is provide it with some training text, which we defined as above. This is the 2005 State of the Union address. And so this will build our custom_sent_tokenizer. So there's that. Now go ahead and click Shift+Enter. So that's all trained up. Come down here. We can now use this to tokenize the sample text. [Author Typing] So we'll use our custom_tokenizer.tokenize the sample text. Alright, there's that alright. So let's go ahead and click Shift+Enter here. And that ran as well. So we're good to go there. So we have our tokenized sentence. Let's maybe print that out as an example just so we can see what it produced. So we have all of these split into the individual sentences. They are in a list. Some of these are longer than others, separated by commas. So all the different sentences in this speech in list form. So that's really helpful. But now let's define a function [Author Typing] that will tag each tokenized word with a part of speech. So this is the part of speech labeling. So we're going to just define this here. We'll define it as process_content. [no audio] And so what we're going to do is we're going to try for i in tokenized. Now let's just do this for the first five, so first five sentences. Because remember, our tokenized is a sentence tokenizer, right now. [Author Typing] We'll have words= nltk.word_tokenizer. [author typing] So we're splitting each of our sentences into individual words. And then we'll have tagged. And so we can do the part of speech tagging with an nltk function. Just pos_tag words. So as easy as that. And let's go ahead and actually print this out as well. So we'll print tagged. So we had a try up above. So if that doesn't work, if it gives us an Exception, [No Audio] we'll just print the string e. [Author Typing] All right, but this is just a function. We need to call this function. [Author Typing] And we do that just using these parentheses here. All right, so let's go ahead and run this cell and see what it produces for us. All right and here we go. So here we have all of these words in each sentence tagged with a part of speech. NNP here is a noun. We also have a lot of other words here. We have JJ, IN, DT, determinant, so maybe if we just do part of pos tagging nltk, we can get some of these words here. [Thinking] So maybe here we go. This could be a good one. [No Audio] So let's try this. That way we can learn some of these, what these part of speech tags mean. [no audio] Because, like, it's pretty obvious that the NNP is a noun. This is producing a list of tuples. The word with is part of speech tag, but some of these are less obvious. How does NNP differ from NN, RB? So let's go ahead and run this. And I actually don't have it downloaded, that's okay. [No Audio] So it's suggesting that I run the NLTK downloader, which is probably a good call. So let's go ahead and try that. [No Audio] We'll do nltk.download. [No Audio] Ahhh, always type in ntl, not sure why. All right, so here's this. [No Audio] So if we actually look back here, we find the tag sets back here in the All Packages. So I'm going to go ahead and download that. So it's installing, finished installing. Perfect, we've got our tag sets. Let's go back and try to run this again and see if we can get better results. Ahh, Perfect. So here we go. So this is telling us all the possible labels we have dollar, some punctuations, we have conjunctions, numeral, determiner, existential there, a foreign word, adjectives, lots of different adjectives, noun, a common noun, a proper noun, a plural proper noun. So these are telling us all of the possible different tags that we could see in this part of speech tagging when we run this function above. So, but here we go. This is actually really sweet. All of these are labeled with their part of speech. This would have come in really convenient in elementary school, but that's all right, we'll use it now. [No Audio] Okay, so now that we've done part of speech tagging, we can move on to chunking with NLTK. So chunking is simply grouping the words into meaningful clusters. The main goal of chunking is to group words into noun phrases, which would be like a noun with any associated verbs, adjectives and adverbs. So the part of speech tags that were generating the previous step will be combined with regular expressions. So if you're not familiar with regular expressions in Python, I'll bring in some common ones for you here. So like a + is going to be a match 1 or more match 0 or 1 repetitions, * match any or more repetitions. So these are going to get used so that we can do chunking or grouping these into these sample text clusters. So I'm actually going to go back here and I am going to copy these because we're going to use these again. So I'm just going to kind of consolidate all of this into one cell. Let me get another one below this. [No Audio] Pull this down. All right, so we're going to have our training in sample text. [author typing] We're going to have our custom tokenizer, which we trained let's get that Ctrl+C, bring this down. [Author Typing] All right, let's actually tokenize the words come back up here. [no audio] Outward and stemming went too far. Let's take this, bring it all down, Ahh! So just consolidating here so that we can see all of this conveniently in one code cell. And then the last one, let's get our process function. [author typing] Here we go. Let's take this Ctrl+C, bring this down. [No Audio] All right, so now we want to do the chunking. So we're going to have to change some of this a little bit [Author Typing] and we're going to use these regular expressions in this process content. All right, so we have four in tokenized and let's actually eliminate this just five. Let's do this for all of them. We're going to do the word tokenized again. We're going to do the part of speech tagging. We won't print this out this time though. So let's combine the part of speech tag with a regular expression. [Author Typing] So this is actually what's going to produce our chunks. So we'll call this a chunkGram and we're going to define it. We're going to use these triple quotations here. We'll have chunk and we use these curly brackets. So there's a lot of kind of notation here, but that's okay. And I'm just going to type this out and then we can talk about what each part means in a second. So just follow along, question mark, get an asterisk, NNP plus NN. Question mark, endcurly bracket and triple quotations. All right, so let me add a content or add a code cell here and I'll actually define what some of these mean. So that line broken down is going to be these. So we have RB.? is going to be zero or more of any tense of adverb because the RB here is going to be an adverb followed by zero or more of any tense verbs and then one or more proper nouns followed by zero or one singular noun. So this is the chunk that we're going to be looking at. This is that noun phrase with the noun and any of its associated verbs. So we're going to pull these out from all of our tokenized sentences, but we're not quite done. We're going to use a chunkParser. So we can have chunkParser is equal to the nltk and this is RegxpParser. And then we'll do chunkGram. [no audio] And finally chunked equals chunkParser.parse, the tagged words that we have above. All right. And we can actually draw these chunks. This is a cool feature of NLTK. [Author Typing] So we'll do chunked.draw and just parentheses there. All right, we've got our process content here still, so we should be good to go here and have all of the chunking that we need. So let's do a Shift+Enter and see how this works. And here is actually the NLTK.draw. So this is separating the words into individual chunks. So here for example, we have President George Bush. So this is a noun phrase. Some of these aren't as informative and this is actually just the first sentence. So this is going to print out all of these for each sentence. [No Audio] Oh! Yep, here we go. It's going to keep going. So maybe I should have said only do the first couple of sentences here, but anyways, this is a cool way to look at the chunks. Maybe not the most useful way to use the chunks. So we'll actually look at a little later how we could access these chunks because they are stored as an nltk tree. If we keep closing this, this will keep going through all of the sentences in the state of the Union, so maybe we don't want to do that. It also shows relationships in these chunks, which is kind of cool, where they're all connected. This s just means sentence. So this only has one noun phrase in this sentence. So instead of going through all of those, the applause, that's all there is in that. I'm going to do this interrupt kernel. Just click stop up here. So this is going to stop it. It says I did a keyboard interrupt. Maybe let's just do instead of all of those, we'll just do the first two or three. If we run this again, we can go through it pretty quickly. [Author Typing] So it has all these [No Audio] it's thinking I clicked, we got some applause. [No Audio] All right, so that's done. We did our first three sentences. Let's go ahead and take this and let's actually look at how we would access those as a, as an NLTK tree because this draw function, although it's really cool to look at and it's nice to see because it helps you kind of understand what's going on, we can't really use that as well. So what I'm going to do is down here before we draw these, I'm going to add in a function. I'm just going to print the nltk tree. And so to do that I'm going to say a for loop here, for subtree in chunked.subtrees. And I'm actually going to use a lambda function here. Lambda functions are a convenient part of Python. You might see them come up from time to time. Instead of defining a whole definition function, we can just do this quick one time use inline function that we're defining. So label equals Chunk. So this is the same for the subtree, if the label is chunked or not chunked, sorry. If it's just chunk, we're going to do the following. And that is just a simple print subtree. So we're going to print out all the subtrees here that are actual chunks. So let's go ahead and take this back and I'm actually going to up this a little bit. Let's do the first ten sentences and we'll go a little higher. Let's do the first 20 sentences. All right, so shift enter here, we'll see how this does, alright. And here we go. So here are all the chunks or the noun phrases out of the first 20 sentences. So some of these are just one words. Some of them have a couple of nouns. President George Bush instead of just President George and Bush being individual words. And let's actually up this farther, see if we get some more. [no audio] So, yeah, President George Bush is a common chunk here. White House photo, Eric Draper. So a lot of stuff in here, but we are kind of able to separate out these noun phrases. We don't want supreme being labeled as an adjective here because supreme is actually proper noun. It's the Supreme Court. So it's good that we've got those together as well as we're separating the individual names into their actual first and last names. So that's very useful. Another good part of preprocessing in NLTK. [no audio] But now we're actually going to move on to chinking with NLTK. So this is very similar to chunking. [Author Typing] So sometimes there are words in the chunks that we don't want. So we can remove them using this process called chinking. And so this is pretty easy. Let's take the same function we defined above. I'm going to take this down. I just Ctrl+C to copy and then CTRL+V, paste these in. So now we want to remove words from these phrases that we don't want. So I'm going to change this chunkGram here a little bit. [Author Typing] So we still have our quotations and then we have our chunk, which is again, the curly brackets. [Author Typing] So we'll take this. And here is actually the chinking part. What we can do is we can define words that we don't want in this phrase by using these inside out brackets or these kind of backwards brackets. So we want to remove verbs. We'll take those out. And then we can use these vertical bars to delineate other word types that we don't want in there. So we don't want determiners. [no audio] So we'll take all these out, get a plus there. And then this last bracket, we only need one and we actually need a third. [Author Typing] There we go. So this is going to take this chunk, but take all these words out of it. So take out all these verbs, preposition, determiners, or the word to actually is here. So here, let me actually pull in this comment from my other notebook that will help explain this a little bit. So the main difference is the bracket. And we're just removing the chink one or more verbs, prepositions, determiners, or the word two. So that's all we're doing in this phrase here. So let's go ahead and do that again and actually print out these, which is good. Maybe let's print the chunks so we can see how it kind of edited it. I'm going to go down to 20 here. All right, so here's our chunk. PRESIDENT, GEORGE BUSH, ADDRESS, BEFORE, blah, blah, blah. And then it prints out kind of the chunks, chunks here without any of the editing. Actually, I'm going to go back. This is kind of hard to look at when we print these out. I'm going to add this comment here and click Shift+Enter again. All right, so here's these chunks now. And this makes a little bit more sense. We have a lot of information in there, but if you notice, none of them are going to have verbs, preposition, determiners, or the word too. [Author Typing] So again, just really helpful if you're trying to remove different words from the chunks that you are collecting or the clusters that you are building from these tagged words. So there's that another aspect of chunking. And then finally, this is actually going to be the last thing that we're going to do in this first video of this project. This is named entity recognition with NLTK. So this is a really common form of chunking in the process. It's called named entity recognition. So NLTK is able to identify people, places, things, locations, monetary figures, and more. So there are two major options with NLTK's, named entity recognition, we can either recognize all named entities or we can recognize or we can recognize named entities as their respective type, like people, place, location, etcetra. And it's going to be just controlled with one simple option. So let me go back up ahead. We'll get our function here again, I'm going to just take this guy. We don't have to retokenize the words each time, I suppose, since we've already done that. So I'm going to take out the chunked grams. So let's take this out. [No Audio] So right after the tagging, we're going to do our namedEnt recognition. So we can have namedEnt= nltk.ne_chunk tagged. And then so here's the option. We have binary equals true or binary equals false. That is all we really need to do. So I'm going to get rid of this lambda function here and simply do a namedEnt.draw. [No Audio] Okay, so let's go ahead and run this and see what it looks like. So here it pulls up a list. So you notice here that we have simply this NE. So notice it's a named entity. So, GEORGE,ADDRESS, CONGRESS, UNION. But if we actually go back and I'm going to have to stop this here, I believe otherwise we're going to go through 20 sentences. Or here is another good one, Mr. Speaker. [No Audio] So let's stop this KeyboardInterrupt. Let's change this binary to False. Go ahead and run this again and we'll look at the difference. Oh, we've got an extra, not sure what happened. There we go. All right. So when we have binary equals False, we're not looking for any named entity. We are going to actually try to label those as person, places or locations. So here we have person GEORGE BUSH. And you notice it separated it into instead of just GEORGE as one named entity. Now we have a PERSON with its first and last name. GEORGE BUSH, ORGANIZATION ADDRESS I'm not sure if that's right. CONGRESS ORGANIZATION, good call. State of the union it's also called in an organization that's all right, it's not perfect. So this is, this is a cool way to kind of pick out these different parts of these different chunks. Identifying people, Identifying organizations. This will also do locations, stuff like that. So that's namedEnt recognition, NLTK makes this really, really convenient. Perhaps you could use this in a situation where you had an email and you wanted to quickly save information such as location, date, time of a meeting that was scheduled in that email. This namedEnt recognition through NLTK could do something like that. All right, so that's that. I'm going to clear this output here. We have namedEnt recognition. Now let's just do a quick pass over what we've gone through so far. So this is using NLTK, which is this convenient package in Python for Natural Language Processing. We looked at tokenizing words, both sentence tokenizing and word tokenizing. So splitting into sentences from a body of text or a corpus, we're splitting into words. We looked at removing stopwords, which is filtering useless data from these tokens. And we did that in a couple of different ways. We also looked at training the stemming with NLTK. So removing tenses and normalizing the words. So stemming is a great way to normalize kind of the tokens that you produce, specifically word tokens. We looked at pulling in different texts from NLTK, example text. We also looked at using the PunkSentenceTokenizer, which is a trainable sentence tokenizer, which we then used in these functions here to actually do part of speech tagging. So in this example, we were able to identify the part of speech of each individual word, the tokenized words. So we have all these different examples that they could be labeled as nouns, adjectives, verbs, pronouns, preposition, adverbs particles, etc. We looked at chunking and using those with regular expressions that come up in Python from time to time. So this is how you perform chunking using the chunkGram and the chunkParser from NLTK. We also looked at how you might remove some of the words that you didn't want. If you and I'm not sure why these are all highlighted here, there we go. We looked at using chinking to remove some of the words from a chunk that we wouldn't necessarily want. So that is a slightly more complicated form of chunking. And then we looked at namedEnt recognition using NLTK's built in namedEnt recognizer. So we looked at labeling different chunks or words in different chunks as people, places, locations, or things. All right, so this is a pretty good start, but we'll actually get into the cool parts in the next video, and that is actually performing text classification. So we're going to do this using NLTK. We're going to use a linear support vector machine to classify movie reviews as either positive or negative based off some of the words or the features that they contain. So keep listening to this next project. We've learned some good basics so far, but we're actually going to be able to apply them, apply them soon. So thanks for listening. I'll talk to you soon.




Hello everybody and welcome to the final video in this project, on Natural Language Processing in Python. We've covered the basics and so now we can actually do text classification where we will deploy a machine learning algorithm, a support vector machine to be specific, to classify movie reviews as either positive or negative. So we're going to have a data set of a bunch of different documents where those documents are movie reviews and we want to have the computer learn whether or not it is a positive review or a negative review just based off the words that are contained therein. So we'll look at how we are going to take tokenized words and then use them as features that we can feed into our machine learning algorithm. We're also going to look at how to wrap the scikit-learn package with NLTK and use classifiers from scikit-learn in Python through NLTK. So let's go ahead and get started. Previously we've covered Tokenizing, Stemming, Part-of-speech tagging, Chunking and Chinking and then Named Entity Recognition. So we're going to use some of these basics, this foundation that we've built to do this text classification. But for now let's go ahead and just import a couple libraries that we need. I'm going to import nltk again just so for readability's sake. And then from the nltk.corpus we're going to import the movie_reviews, and we need an underscore there. So movie_reviews, so that's the set of documents that we're going to be using. And remember, if you don't have the movie_reviews installed already, you can go nltk.download and this is going to pull up an NLTK GUI that you'll be able to download those from. So this is sneaking a little bit. It's still processing here as noted by this asterisk. Hopefully these will import correctly. There we go, we see the one so it worked. And we're going to be using the random to set a random seed so that we can improve our reproducibility a little bit. Again, I'll just run this really quick so that you can see if you were following around in the, following along in the other project videos, as I hope you were, then you already understand how the nltk.downloader works. But if not, here you go, you can go to the Corpora, all the different documents and in here you can find the movie_reviews right here actually and get that downloaded, installed. It should be relatively quick, it's just a zip file. All right, so now that we have that, let's compile a list of our documents. [No Audio] So we're going to build a list of documents. So to do that we are going to use the movie_reviews that we just imported. We want them as a list. So we have movie_reviews.words, fileid, category. So we're going to use an inline function here because these are a bunch of different reviews. So what we can do is we can do a for category in movie _reviews.categories. So these two categories are actually going to be positive and negative here. They're split into those different categories and then for fileid in movie_reviews.fileids for each category. So there we go. This is going to build that list of documents both positive and negative reviews. And once we have this imported, we can go ahead and explore this a little bit. But let's keep building this. We want to shuffle the documents. That way we don't have all of the positive reviews and negative reviews in order that could skew our results. So let's use the random.shuffle documents. [No Audio] Okay, there's that. Let's actually start printing out a few things so we can learn more about this data set that we're building. So let's go ahead and print out the total Number of Documents. I'm just going to use a length documents so it's going to tell me how long that list is. Let's also print out the First Review so we can see what these look like .format documents. All right, here we go. We'll do the first one here. [No Audio] Okay, that should be good. All right. And now what we want to do is we want to build a list of all the words contained in because we want to use these words as features so we need an entire list of all the words not just split into these different documents. So we're going to do that in a very similar way as we did the documents. We're going to use a for loop, so for w in movie_reviews.words parentheses here, we're going to do all_words.append w, which is our word here. And then we'll do a lower to make sure it's a lower case. All right, that's awesome. This is a list of all the words. And then we'll take that list all_words and we can do a convenient nltk.FreqDist of all those words. So this is going to sort them by most common to least common, and it's doing that using this FreqDist function built into the nltk library. So awesome, let's take a look at some of those. So we'll print out Most common words. Let's get these all_words.most_common. Let's add 15 here. That's good. And let's print, how about the word happy? [Author Typing], And we're going to print out how many times happy comes up. So format all_words, and we can use the key index here of happy. So this is going to tell us how many times happy comes up in all_words. All right, so if we have everything correct, we can go ahead and run this Shift+Enter, and so this is going to build a list of the documents, shuffle them, and then generate this combined list of all the words in these movie_reviews. So this is going to take a little bit because there are, well I'll just tell you there's 2000 documents in this data set. So it's got to run through each and every one of those, and it also needs to append a list of all the words, it's got to sort them, and it's also doing these print functions as well. So here we go, though it, it actually didn't take that long. I've spelled documents right, wrong up here. That's going to bug me a little bit. That's okay. So Number of Documents, 2000. Here's the first review. It shows that America remains ambivalent over the nature of its political system. Probably true. But these are all tokenized into words. It's got the entire review here. So most common words, we have most common is actually punctuation. And then we get into the, the we have another period, a so that makes sense, you know commas and periods, and the words a, we, and and obviously going to come up a lot of the time. So here these are. And actually the word happy shows up 215 times in these reviews. So that's a good thing. It's good to be happy. So there you have it. So this is the data set that we're going to be using. Let's take a look at converting these words over to features. So let's, just out of curiosity sake, let's print the length of all_words. Let's see how many we have here. [Author Typing] So we have 39,000 words, 768. So a lot of input, but we don't need all of these. We aren't going to use 39,000 different features to train our machine learning algorithm on. So what we're actually going to do is use only some of the most common ones. So let's use the 4000 most common words as features. So word_features=list, all_words.keys and let's go ahead and take the first 4000 of these. So this is an interesting way to build features. These may not be the most informative features, so we could have ways to improve this in the future. But for now when we're just looking at a basic introduction to text classification, we're not going to bother with processing our data set too extensively yet. So we'll take these features and we'll assume that they are a good way to go. All right, so there's that. Let's run Shift+Enter on this. That runs pretty quick. So now we have our word_features, we have 4000 features and then let's build a find_features function that will determine which of the 4000 word features are contained in a review. So here we go. For each document we want this function to be able to sort through it and determine which of the features are in the review. So we'll do a find_features and it's going to take an input of document which is going to be a review in this case. So words, let's take set document and the documents already have the words tokenized, so we don't have to do that. So here are our words. Let's build up features and we're going to start a dictionary. We're using brackets here. So this is going to be a blank dictionary you know in Python you have a list, the brackets, the curly brackets are going to give you a dictionary. All right, now we can loop through these. So we'll do for w in word_features. features[w] equals w in words. So if that word is contained in the list of words in our document, we are going to start building this dictionary of it where the key is going to be a w or the word, and the value is going to be a true or false type scenario. All right, let's return these features, [Author Typing] and then let's use an example. So let's use an example from a negative review. So this is what the features is going to look like. features, we can just call this function now find_features and we'll do movie_reviews. So we want to pass it just one document, do words. So I already know the file name. This is coming from the negative category and then cv000_29416.txt. So this is just one of the reviews. And then let's actually print these out. So for key, value in features.items. if value equals True, [Author Typing] print the key. Oh, we need a colon here, print the key, all right. So let's go ahead and run this. And that was really quick. We've already got the output here. So in this review we have the following features. We see that we get like the periods, because of course the period was one of the most common words, but we also have a lot of other descriptive words in here. Perhaps problem could give away that it's a negative review, horror, perhaps confusing, this could be a dead giveaway that this should be a negative review. But here are features. This entire list of features is a binary True or False for each feature as to whether or not the word is contained in that document. So this is what we're going to be feeding into our machine learning algorithm, or our support vector machine in this case. But again, this is just for the one particular document. So maybe we could actually print these out. Let's just print the features dictionary that we just generated so we can see what that looks like. So this is going to take a little bit longer, but you see, so for the majority of these words, and this isn't in order for most common to least dictionary does not store order in Python. So these are randomly shuffled. You have for each word a True or False as to whether or not it's contained. So since we have 4000 words here and only maybe 20 or so were found in the document, most of these are False. [No Audio] But again, there are going to be a couple of Trues mixed in. So let's go ahead and clear this output. We know what that looks like. Let's go ahead and do this for all the documents. [No Audio] For all the documents. Yo know, we had our one example text. Now let's build all of our inputs. So featuresets, we'll call these featuresets and we're going to do another inline for loop. So we'll have find_features of review for comma category, for review, category in documents. So for both the positive and negative reviews, we're going to find the features for each review in our documents, which was the list of all the documents that we generated up above here, right here. So we built that list of documents and now we're going to build this feature set off of them. All right, so let's go ahead and run this, make sure it works correctly. [No Audio] So this is going to be a pretty big variable here, this featuresets and it did run, we see that number there. So now we're going to start importing sklearn and we're going to use the model_selection to import the train_ test_split function which we've used in the past to build a training and testing data set. So let's go ahead and do that. So we can split the featuresets into training and testing datasets using sklearn. So we are going to from sklearn import the model_selection, which again we've used in the past. Let's define a seed for reproducibility. So we'll say seed=1, and actually perhaps when we shuffled the documents up here before, we should have set our seed before that. But that's okay, we'll just do it now. All right, so seed=1, and now let's actually split the data into training and testing datasets. So training, testing = model_selection.train_test_split featuresets and let's define test_size, test_size is 0.25. Since we have a lot of data, we can have a big validation dataset and then random_state=seed. So there we go. If you've noticed, you might be kind of confused by this because in the past when we've used this train_ test_split function, we've actually had four outputs. We've had the X training, the Y training and then the X testing and the Y testing. And this is actually going to be because of the way that we have to pass these training and testing data sets through the nltk to the sklearn. We don't need to pull off the class or the positive or negative tag from these training and testing data sets yet. And so since we don't have an X and a Y here, we only have featuresets, this is just going to split this into training and testing. So we'll look a little bit later about why that's different. So let's go ahead and run this and it's already done. So just for fun, let's print the training length and then print the length testing. If everything went well, we should get 1500 in the training. Opps, I have a spelling error here. There we go. So 1500 in the training, 500 in the testing. This makes sense. We had a test size of 25%. So this is good. We have 500 documents, 500 reviews to test our algorithm on, and we have 1500 train it on. So now let's look at how we, [no audio] How we use sklearn algorithms in NLTK. So we're going to need to import a couple of things. So from nltk.classify.scikilearn, let's import the SklearnClassifier. And then from sklearn.svm, let's import that support vector classifier which we've come to know and love in previous projects. Let's go ahead and run this. So everything works. We already have these installed. That's good. All right. So now we have to start an instance of our SKlearnClassifier. So model is going to equal the SKlearnClassifier. And inside of this we are going to use our standard SKlearn class, which is the SVC. And let's define a kernel here. I'm going to use a linear kernel. We could test out some others if we wanted, but linear is a good place to start. So we have to, each time we use these scikitlearn classifiers with NLTK, we have to wrap it in this SKlearnClassifier class. So we have to use this first and then this is what we are going to be passing our information to later, our training and testing datasets. So Shift+Enter. That ran good, we're good to go. Let's move on so we can train the model on the training data. Step number one. So our model, our SVC, wrapped in the SKlearnClassifier, we can just do simple dot train training, pass it all of the training data, no need to separate the class here. And we'll Shift+Enter. And this will think for a little bit. We're spitting that support vector classifier to this data, trying to find that optimal separating hyperplane, which is going to divide these different documents into either positive reviews or negative reviews. [No Audio] And there we go. So let's go ahead. This is all run, but remember, information from our training doesn't necessarily tell us that much. So let's go ahead and test on the testing dataset. So let's see how well we are able to generalize to documents that we haven't seen before. So let's do accuracy= nltk.classify.accuracy. We'll pass it the model which is fully trained, and then we'll pass it the testing dataset. And let's go ahead and print that out. So we'll print the SVC Accuracy. [Author Typing] We'll use the dot format to substitute in a variable into our string here, accuracy and parentheses. All right, so let's see how we did. So, going through the testing dataset, we are thinking, thinking, testing, classifying documents, [No Audio] still thinking. And here we go. So, 63% accuracy certainly not the most accurate classifier that we have seen in any of our projects to date. But that's okay because we are just getting started. There are tons of different ways that we can improve this result. But for now, we're just trying to build that foundation for Natural Language Processing so we don't have to go into those details too much yet, but we will in a future project so that's okay. So we can learn how to improve these results later that's fine. In this project, we built a foundation for Natural Language Processing in Python. We covered tokenizing, stemming, part of speech tagging, chunking, named entity recognition, and finally we just did text classification. So some of those ways that we'll look at improving this result in future projects will be at combining multiple classification algorithms so we don't have to use just a support vector machine. We can combine this with perhaps a naive based classifier or other classifiers to produce better results. Furthermore, we'll move on to more difficult challenges, such as sentiment analysis. You know, is the overall review positive or negative? Is a pretty easy sentiment to classify, and we'll look at some more difficult scenarios in the future. But thanks for following along. You have built your foundation. This is actually the end of the project. We covered all the basics, the tokenizing, we learned a little bit about some vocabulary in Natural Language Processing, and we built all the way up into doing a simple text classification of either a positive or negative movie review based off feature set that we built from all of these documents. So, again, using the most common words as features, these aren't going to be the most descriptive since all the documents have probably a comma and a period, and so we'll look at a better way to build features later. But for now, the big takeaway here is probably how to import this SklearnClassifier and use it to deploy Sklearn algorithms through this NLTK package. All right, thanks very much for listening. I hope you learned a lot from this project. [No Audio]

